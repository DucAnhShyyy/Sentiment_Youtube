{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783181cd",
   "metadata": {},
   "source": [
    "**Note**: Lexicon- based là 1 phương pháp riêng nên sẽ được chạy ở notebook này để dễ kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a449a881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import string\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0cb980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04df3053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afeccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube API setup\n",
    "# Replace with your API key\n",
    "api_key = os.getenv('YOUTUBE_API_KEY')\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8c83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_video_id(url):\n",
    "    \"\"\"\n",
    "    Extract the video ID from a YouTube URL.\n",
    "    Args:\n",
    "    url (str): The YouTube video URL.\n",
    "    Returns:\n",
    "    str: The video ID or None if invalid URL.\n",
    "    \"\"\"\n",
    "    # Regular expression to match YouTube video IDs\n",
    "    video_id_match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    if video_id_match:\n",
    "        return video_id_match.group(1)\n",
    "    else:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c4ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ID: 5LxjFoypQo4\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.youtube.com/watch?v=5LxjFoypQo4'\n",
    "video_id = extract_video_id(url)\n",
    "print(\"Video ID:\", video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbe3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = '5LxjFoypQo4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ed0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment of the comments\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return sentiment, compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12db6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_1(video_id):\n",
    "    comments = []\n",
    "    \n",
    "    # Make an API request to fetch comments\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText'\n",
    "    )\n",
    "    \n",
    "    while request:\n",
    "        response = request.execute()\n",
    "\n",
    "        # Loop through the comment threads and extract the comment text\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comments.append(comment)\n",
    "\n",
    "        # Get the next page of comments (if any)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa6ed07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape comments, perform sentiment analysis, and write results to CSV\n",
    "def scrape_and_analyze():\n",
    "    # Get comments from the YouTube video\n",
    "    comments = get_comments_1(video_id)\n",
    "\n",
    "    # Write comments to a text file\n",
    "    with open('comments.txt', 'w', encoding='utf-8') as file:\n",
    "        for index, comment in enumerate(comments, 1):\n",
    "            file.write(f\"{index}. {comment}\\n\")\n",
    "\n",
    "    # Perform sentiment analysis on all the comments\n",
    "    all_comments = ' '.join(comment.strip() for comment in comments)\n",
    "    overall_sentiment, sentiment_score = analyze_sentiment(all_comments)\n",
    "\n",
    "    # Get current timestamp\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Define the CSV file and column names\n",
    "    csv_file = 'comments_analysis.csv'\n",
    "    fieldnames = ['Timestamp', 'Comment Count', 'Sentiment Type', 'Sentiment Score', 'Video Type']\n",
    "\n",
    "    # Open the CSV file to write data\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # If the file is empty, write the header (column names)\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the timestamp, total comments fetched, sentiment type, and score\n",
    "        writer.writerow({\n",
    "            'Timestamp': timestamp, \n",
    "            'Comment Count': len(comments),\n",
    "            'Sentiment Type': overall_sentiment, \n",
    "            'Sentiment Score': sentiment_score,\n",
    "            'Video Type': overall_sentiment\n",
    "        })\n",
    "\n",
    "    # Output the result\n",
    "    print(f\"Time: {timestamp} | Sentiment: {overall_sentiment} | Sentiment score: {sentiment_score}\")\n",
    "\n",
    "    return timestamp, len(comments), overall_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230b558",
   "metadata": {},
   "source": [
    "Có giới hạn (limit) về truy cập API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7811369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-05-06 00:26:00 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:27:35 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:29:12 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:30:46 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:32:21 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:33:58 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:35:33 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:37:07 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:38:42 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:40:18 | Sentiment: Negative | Sentiment score: -0.9998\n",
      "Time: 2025-05-06 00:41:52 | Sentiment: Negative | Sentiment score: -0.9998\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=5LxjFoypQo4&textFormat=plainText&key=AIzaSyCAgYDsQbQ1nevp81yUudK6kHQT7Rbfj2g&alt=json&pageToken=Z2V0X25ld2VzdF9maXJzdC0tQ2dnSWdBUVZGN2ZST0JJRkNJa2dHQUFTQlFpSElCZ0FFZ1VJcUNBWUFCSUZDSWdnR0FBU0JRaWRJQmdCSWc0S0RBaXZydGI3QlJEZ3M5RGtBUQ%3D%3D returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the scraper once and then repeat it every 15 minutes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     timestamp, comment_count, sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_and_analyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform scraping and analysis\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)  \u001b[38;5;66;03m# Sleep for 15 minutes (900 seconds)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36mscrape_and_analyze\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_and_analyze\u001b[39m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Get comments from the YouTube video\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     comments \u001b[38;5;241m=\u001b[39m \u001b[43mget_comments_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Write comments to a text file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mget_comments_1\u001b[1;34m(video_id)\u001b[0m\n\u001b[0;32m      5\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mcommentThreads()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[0;32m      6\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     videoId\u001b[38;5;241m=\u001b[39mvideo_id,\n\u001b[0;32m      8\u001b[0m     textFormat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplainText\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m request:\n\u001b[1;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Loop through the comment threads and extract the comment text\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=5LxjFoypQo4&textFormat=plainText&key=AIzaSyCAgYDsQbQ1nevp81yUudK6kHQT7Rbfj2g&alt=json&pageToken=Z2V0X25ld2VzdF9maXJzdC0tQ2dnSWdBUVZGN2ZST0JJRkNJa2dHQUFTQlFpSElCZ0FFZ1VJcUNBWUFCSUZDSWdnR0FBU0JRaWRJQmdCSWc0S0RBaXZydGI3QlJEZ3M5RGtBUQ%3D%3D returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "# Run the scraper once and then repeat it every 15 minutes\n",
    "while True:\n",
    "    timestamp, comment_count, sentiment = scrape_and_analyze()  # Perform scraping and analysis\n",
    "    time.sleep(60)  # Sleep for 15 minutes (900 seconds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
